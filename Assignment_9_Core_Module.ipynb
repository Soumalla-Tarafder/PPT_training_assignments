{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoiMo3chmDefzyjnlTc54n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumalla-Tarafder/PPT_training_assignments/blob/main/Assignment_9_Core_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the difference between a neuron and a neural network?\n",
        "2. Can you explain the structure and components of a neuron?\n",
        "3. Describe the architecture and functioning of a perceptron.\n",
        "4. What is the main difference between a perceptron and a multilayer perceptron?\n",
        "5. Explain the concept of forward propagation in a neural network.\n",
        "6. What is backpropagation, and why is it important in neural network training?\n",
        "7. How does the chain rule relate to backpropagation in neural networks?\n",
        "8. What are loss functions, and what role do they play in neural networks?\n",
        "9. Can you give examples of different types of loss functions used in neural networks?\n",
        "10. Discuss the purpose and functioning of optimizers in neural networks.\n",
        "11. What is the exploding gradient problem, and how can it be mitigated?\n",
        "12. Explain the concept of the vanishing gradient problem and its impact on neural network training.\n",
        "13. How does regularization help in preventing overfitting in neural networks?\n",
        "14. Describe the concept of normalization in the context of neural networks.\n",
        "15. What are the commonly used activation functions in neural networks?\n",
        "16. Explain the concept of batch normalization and its advantages.\n",
        "17. Discuss the concept of weight initialization in neural networks and its importance.\n",
        "18. Can you explain the role of momentum in optimization algorithms for neural networks?\n",
        "19. What is the difference between L1 and L2 regularization in neural networks?\n",
        "20. How can early stopping be used as a regularization technique in neural networks?\n",
        "21. Describe the concept and application of dropout regularization in neural networks.\n",
        "22. Explain the importance of learning rate in training neural networks.\n",
        "23. What are the challenges associated with training deep neural networks?\n",
        "24. How does a convolutional neural network (CNN) differ from a regular neural network?\n",
        "25. Can you explain the purpose and functioning of pooling layers in CNNs?\n",
        "26. What is a recurrent neural network (RNN), and what are its applications?\n",
        "27. Describe the concept and benefits of long short-term memory (LSTM) networks.\n",
        "28. What are generative adversarial networks (GANs), and how do they work?\n",
        "29. Can you explain the purpose and functioning of autoencoder neural networks?\n",
        "30. Discuss the concept and applications of self-organizing maps (SOMs) in neural networks.\n",
        "31. How can neural networks be used for regression tasks?\n",
        "32. What are the challenges in training neural networks with large datasets?\n",
        "33. Explain the concept of transfer learning in neural networks and its benefits.\n",
        "34. How can neural networks be used for anomaly detection tasks?\n",
        "35. Discuss the concept of model interpretability in neural networks.\n",
        "36. What are the advantages and disadvantages of deep learning compared to traditional machine learning algorithms?\n",
        "37. Can you explain the concept of ensemble learning in the context of neural networks?\n",
        "38. How can neural networks be used for natural language processing (NLP) tasks?\n",
        "39. Discuss the concept and applications of self-supervised learning in neural networks.\n",
        "40. What are the challenges in training neural networks with imbalanced datasets?\n",
        "41. Explain the concept of adversarial attacks on neural networks and methods to mitigate them.\n",
        "42. Can you discuss the trade-off between model complexity and generalization performance in neural networks?\n",
        "43. What are some techniques for handling missing data in neural networks?\n",
        "44. Explain the concept and benefits of interpretability techniques like SHAP values and LIME in neural networks.\n",
        "45. How can neural networks be deployed on edge devices for real-time inference?\n",
        "46. Discuss the considerations and challenges in scaling neural network training on distributed systems.\n",
        "47. What are the ethical implications of using neural networks in decision-making systems?\n",
        "48. Can you explain the concept and applications of reinforcement learning in neural networks?\n",
        "49. Discuss the impact of batch size in training neural networks.\n",
        "50. What are the current limitations of neural networks and areas for future research?\n"
      ],
      "metadata": {
        "id": "m5h_Grhh5NFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. neural nw is a collection of multiple neurons.\n",
        "Neuron we can consider as collection of multiple mathametical operations."
      ],
      "metadata": {
        "id": "wCEvTuH15OhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2.\n",
        "components are a activation fubtion,a weight,data,a bais.\n",
        "\n",
        "all the data multipy with weights then add a bais with the output and pass the entire data with a activation function and we\n",
        "will get the outout."
      ],
      "metadata": {
        "id": "YvR3ejUM55sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. singel layer nn with one singel neuron called perception and perception is not able to find non linear pattern from data\n",
        "\n",
        "collection of perceptron form a multilayer perceptron. multilayer perception is  able to find non linear pattern from data"
      ],
      "metadata": {
        "id": "eAN2ENcC6dfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.\n",
        "Forward propagation is where input data is fed through a network, in a forward direction, to generate an output. The data is accepted by\n",
        "hidden layers and processed, as per the activation function, and moves to the successive layer."
      ],
      "metadata": {
        "id": "FWVGhN1NfQQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.\n",
        "Backpropagation is a process involved in training a neural network. It involves taking the error rate of a forward propagation and\n",
        "feeding this loss backward through the neural network layers to fine-tune the weights. Backpropagation is the essence of neural net training"
      ],
      "metadata": {
        "id": "AM2mHBw5fQNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.\n",
        "The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass\n",
        "through a network, backpropagation performs a backward pass while adjusting the model's parameters (weights and biases)"
      ],
      "metadata": {
        "id": "MmITwJJ0fQJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8.\n",
        "A loss function measures how good a neural network model is in performing a certain task, which in most cases is regression or classification.\n",
        "We must minimize the value of the loss function during the backpropagation step in order to make the neural network better."
      ],
      "metadata": {
        "id": "sRUqQvrOfQHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.\n",
        "Loss Functions in Neural Networks\n",
        "Mean Absolute Error (L1 Loss)\n",
        "Mean Squared Error (L2 Loss)\n",
        "Huber Loss.\n",
        "Cross-Entropy\n",
        "Relative Entropy\n",
        "Squared Hinge."
      ],
      "metadata": {
        "id": "7ngcGDrPfQEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10.\n",
        "An optimizer is an algorithm or function that adapts the neural networks attributes, like learning rate and weights. Hence, it assists in\n",
        "improving the accuracy and reduces the total loss."
      ],
      "metadata": {
        "id": "rTLmsm-sfQBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11.\n",
        "one primary cause of gradients exploding lies in too large of a weight initialization and update, and this is the reason why gradients in our\n",
        "regression model exploded. Hence, initializing model weights properly is the key to fix this exploding gradients problem."
      ],
      "metadata": {
        "id": "SreG4HZcfP_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12.\n",
        "In Machine Learning, the Vanishing Gradient Problem is encountered while training Neural Networks with gradient-based methods (example, Back\n",
        "Propagation). This problem makes it hard to learn and tune the parameters of the earlier layers in the network."
      ],
      "metadata": {
        "id": "K45CyivMfP8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13.\n",
        "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus,\n",
        "Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear\n",
        "equation."
      ],
      "metadata": {
        "id": "AMMY0O3VfP5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14.\n",
        "Normalization can help training of our neural networks as the different features are on a similar scale, which helps to stabilize the gradient\n",
        "descent step, allowing us to use larger learning rates or help models converge faster for a given learning rate."
      ],
      "metadata": {
        "id": "4EniLMm_ioRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.\n",
        "Linear Function.\n",
        "Sigmoid.\n",
        "Tanh.\n",
        "ReLU.\n",
        "Leaky ReLU.\n",
        "Parameterised ReLU.\n",
        "Exponential Linear Unit."
      ],
      "metadata": {
        "id": "uKJ331eJioOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16.\n",
        "Batch normalization is a technique to standardize the inputs to a network, applied to ether the activations of a prior layer or inputs directly.\n",
        "Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization\n",
        "error."
      ],
      "metadata": {
        "id": "ZQIyGZslioJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17.\n",
        "\n",
        "Weight initialization is used to define the initial values for the parameters in neural network models prior to training the models on a dataset.\n",
        "How to implement the xavier and normalized xavier weight initialization heuristics used for nodes that use the Sigmoid or Tanh activation functions."
      ],
      "metadata": {
        "id": "GylD7FClioGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18.\n",
        "Momentum is an extension to the gradient descent optimization algorithm that allows the search to build inertia in a direction in the search\n",
        "space and overcome the oscillations of noisy gradients and coast across flat spots of the search space."
      ],
      "metadata": {
        "id": "R_f5aSC2ioC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20.\n",
        "L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights."
      ],
      "metadata": {
        "id": "mYvWTYswioAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21.\n",
        "The learning rate, which governs how often the weights of the network are changed, dictates the magnitude of the update made to the weights.\n",
        "The convergence speed and solution quality are highly dependent on the learning rate"
      ],
      "metadata": {
        "id": "C26xglHYin9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22.\n",
        "Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer\n",
        "outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to\n",
        "the preceding layer."
      ],
      "metadata": {
        "id": "k2KJl2c9in6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23.\n",
        "A final challenge of neural networks and deep learning is the difficulty of generalizing and transferring their knowledge and skills to\n",
        "new or different domains or tasks. Neural networks tend to overfit the data they are trained on, which means they perform well on the training\n",
        "data but poorly on unseen or novel data."
      ],
      "metadata": {
        "id": "S5h8WMTuin3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24.\n",
        "the main difference is that CNN uses convolution operation to process the data, which has some benefits for working with images. In that way,\n",
        "CNNs reduce the number of parameters in the network"
      ],
      "metadata": {
        "id": "pRihtC2Nin0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25.\n",
        "In a convolutional neural network, pooling layers are applied after the convolutional layer. The main purpose of pooling is to reduce the size of\n",
        "feature maps, which in turn makes computation faster because the number of training parameters is reduced."
      ],
      "metadata": {
        "id": "AaWIX4F4inpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "31.\n",
        "The input features are passed through the input layer of the DNN and then processed by the hidden layers, which use non-linear activation\n",
        "functions to learn complex relationships in the data."
      ],
      "metadata": {
        "id": "2NROF0_jk_rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "32.\n",
        "However, training a model involves a lot of challenges from overfitting and underfitting to slow convergence and vanishing gradients\n",
        "many factors can impact the performance and reliability of a deep learning model."
      ],
      "metadata": {
        "id": "PghE75Gik_oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "33.\n",
        "Transfer learning allows developers to circumvent the need for lots of new data. A model that has already been trained on a task for which\n",
        "labeled training data is plentiful will be able to handle a new but similar task with far less data. There are other benefits of transfer learning\n",
        "through deep learning as well."
      ],
      "metadata": {
        "id": "1qx74NEjk_l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "34.\n",
        "This is possible using a deep anomaly detection model. In particular, ScoleMans can use an autoencoder or GAN-based model built with\n",
        "convolutional neural network blocks (see Chapter 3. Deep Learning for Anomaly Detection for more information) to create a model of normal data\n",
        "based on images of normal panels."
      ],
      "metadata": {
        "id": "_vEW_OtKk_ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "35.\n",
        "DNN are based on simple mathematical operations, however, the combination of neurons with non-linear activations over several hidden layers is\n",
        "leading to models that cannot be evaluated with a mathematical formula. This property of the DNN is called a lack of transparency by Roscher."
      ],
      "metadata": {
        "id": "9rvItE0Vk_g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "36.\n",
        "Deep learning has several advantages over traditional machine learning methods, including automatic feature learning, handling large and complex\n",
        "data, improved performance, handling non-linear relationships, handling structured and unstructured data, predictive modeling, handling missing\n",
        "data, handling sequential data"
      ],
      "metadata": {
        "id": "Rh9PrTKBk_dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "37.\n",
        "Generally, ensemble learning involves training more than one network on the same dataset, then using each of the trained models to make a\n",
        "prediction before combining the predictions in some way to make a final outcome or prediction."
      ],
      "metadata": {
        "id": "1UBW_GbBk_aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "39.\n",
        "Self-supervised learning is a machine learning process where the model trains itself to learn one part of the input from another part of\n",
        "the input. It is also known as predictive or pretext learning. In this process, the unsupervised problem is transformed into a supervised\n",
        "problem by auto-generating the labels."
      ],
      "metadata": {
        "id": "tyoy2sYrk_Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "40.\n",
        "Use the right evaluation metrics.\n",
        "Resample the training set.\n",
        "Use K-fold Cross-Validation in the Right Way.\n",
        "Ensemble Different Resampled Datasets.\n",
        "Resample with Different Ratios.\n",
        "Cluster the abundant class.\n",
        "Design Your Models."
      ],
      "metadata": {
        "id": "QowZ9t6jk_U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "42.\n",
        "One of the most important trade-offs is between complexity and generalization. Complexity refers to how well a model can fit the data and\n",
        "capture the nuances and patterns. Generalization refers to how well a model can perform on new and unseen data and avoid overfitting or\n",
        "underfitting."
      ],
      "metadata": {
        "id": "JWLnUlvAk_Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "43.\n",
        "Deleting Rows with missing values.\n",
        "Impute missing values for continuous variable.\n",
        "Impute missing values for categorical variable.\n",
        "Other Imputation Methods.\n",
        "Using Algorithms that support missing values.\n",
        "Prediction of missing values."
      ],
      "metadata": {
        "id": "hv6yAAQTk_M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "45.\n",
        "In edge AI deployments, the inference engine runs on some kind of computer or device in far-flung locations such as factories, hospitals,\n",
        "cars, satellites and homes. The inference is executed on edge devices (e.g.microcomputers, accelerators, mobiles, IoT). A typical example is\n",
        "self-driving cars. They gather information about the surrounding area through multiple sensors and process it in local hardware in real-time12.\n",
        "\n",
        "Software frameworks that optimize a trained neural network (NN) model through weight clustering and pruning, weight and input-output\n",
        "quantization  to fewer bits, fusing NN layers etc., for more efficient execution of NN inferences on edge platforms, play an important role\n",
        "in making machine  learning at the edge (namely EdgeML) a reality3."
      ],
      "metadata": {
        "id": "8BsaPLc0k_KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "46.\n",
        "Distributed training of neural networks is a challenging problem that needs to scale to hundreds of compute nodes to be feasible.\n",
        "Early work in training large distributed neural networks focused on schemes for partitioning networks over multiple cores, often referred\n",
        "to as model parallelism. Interesting techniques include distributed gradient optimization, online learning with delayed updates and hashing\n",
        "and simplification of kernels. Such techniques can be utilized to train very large scale deep neural networks spanning several machines or to\n",
        "efficiently utilize several GPUs on a single machine1"
      ],
      "metadata": {
        "id": "lIo0PA7Linms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "49.\n",
        "The batch size affects some indicators such as overall training time, training time per epoch, quality of the model, and similar.\n",
        "Usually, we chose the batch size as a power of two, in the range between 16 and 512. But generally, the size of 32 is a rule of thumb and a\n",
        "good initial choice."
      ],
      "metadata": {
        "id": "FuXCGUMMfP2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "50.\n",
        "Neural networks are vulnerable to subtle perturbations or modifications of the input data, which can cause them to produce incorrect or\n",
        "misleading outputs. For example, adding a small amount of noise or changing a few pixels in an image can fool a neural network into misclassifying\n",
        "it as a different object."
      ],
      "metadata": {
        "id": "LVo-Q2ULqt8s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}